# ETL-Pipeline-using-PySpark
An ETL (Extract,Transform,Load) pipeline extracts data from sources, transforms it, and loads it into a storage system. this process helps create clean, usable data formats for further data analysis.


PySpark is used for building ETL pipelines for large scale data processing. It offers distributed computing, high performance, and handles structured and as well as unstructured data efficiently.

